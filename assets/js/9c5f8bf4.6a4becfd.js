"use strict";(self.webpackChunkneuraltrust_docs=self.webpackChunkneuraltrust_docs||[]).push([[4226],{86515:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"ai-gateway/plugins/toxicity-detection","title":"Toxicity Detection","description":"Overview","source":"@site/docs/ai-gateway/plugins/toxicity-detection.md","sourceDirName":"ai-gateway/plugins","slug":"/ai-gateway/plugins/toxicity-detection","permalink":"/ai-gateway/plugins/toxicity-detection","draft":false,"unlisted":false,"editUrl":"https://github.com/NeuralTrust/neuraltrust/blob/main/docs/ai-gateway/plugins/toxicity-detection.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Data Masking","permalink":"/ai-gateway/plugins/data-masking"},"next":{"title":"API","permalink":"/category/api"}}');var s=t(74848),r=t(28453);const o={sidebar_position:6},l="Toxicity Detection",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Features",id:"features",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Content Analysis",id:"content-analysis",level:3},{value:"Threshold Evaluation",id:"threshold-evaluation",level:3},{value:"Action Execution",id:"action-execution",level:3},{value:"Configuration Examples",id:"configuration-examples",level:2},{value:"Basic Configuration",id:"basic-configuration",level:3},{value:"Advanced Configuration",id:"advanced-configuration",level:3},{value:"Violence Detection Example",id:"violence-detection-example",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Threshold Configuration",id:"threshold-configuration",level:3},{value:"Security Considerations",id:"security-considerations",level:3},{value:"Example Usage",id:"example-usage",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"toxicity-detection",children:"Toxicity Detection"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Toxicity Detection"})," plugin is a sophisticated content moderation layer designed to analyze and filter potentially harmful or inappropriate content in API requests. It leverages OpenAI's moderation API to detect various categories of toxic content and can be configured to take specific actions when such content is detected."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Categories available in the ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/moderation",children:"OpenAI Moderation API"})," for content analysis:"]})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Sexual Content/Harassment"}),(0,s.jsx)(n.th,{children:"Violence"}),(0,s.jsx)(n.th,{children:"Hate Speech"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sexual"}),(0,s.jsx)(n.td,{children:"violence"}),(0,s.jsx)(n.td,{children:"hate"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sexual/minors"}),(0,s.jsx)(n.td,{children:"self-harm"}),(0,s.jsx)(n.td,{children:"hate/threatening"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"harassment"}),(0,s.jsx)(n.td,{children:"self-harm/intent"}),(0,s.jsx)(n.td,{children:"illicit"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"harassment/threatening"}),(0,s.jsx)(n.td,{children:"violence/graphic"}),(0,s.jsx)(n.td,{children:"-"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"self-harm/instructions"}),(0,s.jsx)(n.td,{children:"-"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"illicit/violent"}),(0,s.jsx)(n.td,{children:"-"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Each category can be individually configured with specific thresholds, allowing for fine-grained control over content moderation policies."}),"\n",(0,s.jsxs)(n.p,{children:["For detailed information about each category and how the OpenAI Moderation API works, please refer to the ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/moderation",children:"OpenAI Moderation Guide"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-Category Detection"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Sexual content detection"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Violence detection"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Hate speech detection"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Customizable thresholds per category"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Flexible Actions"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Configurable response actions"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Custom error messages"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Block or allow decisions"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenAI Integration"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Powered by OpenAI's moderation API"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Real-time content analysis"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 High accuracy detection"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Request Stage Processing"}),":\n\u2022 Pre-request content analysis"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Configurable priority in plugin chain"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Non-blocking architecture"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,s.jsx)(n.h3,{id:"content-analysis",children:"Content Analysis"}),"\n",(0,s.jsx)(n.p,{children:"The plugin analyzes incoming requests by examining the content for various types of toxic or inappropriate material. It processes the content through OpenAI's moderation API and evaluates the results against configured thresholds:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'// Example Request Content\n{\n    "messages": [\n        {\n            "role": "user",\n            "content": "Let\'s discuss this topic respectfully"\n        }\n    ]\n}\n\n// OpenAI Moderation API Response (Internal)\n{\n    "results": [\n        {\n            "category_scores": {\n                "sexual": 0.0001,\n                "violence": 0.0002,\n                "hate": 0.0001\n            }\n        }\n    ]\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"threshold-evaluation",children:"Threshold Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Each category has its own configurable threshold. Content is blocked if any category's score exceeds its threshold:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n    "thresholds": {\n        "sexual": 0.3,    // Block if sexual content score > 0.3\n        "violence": 0.5,  // Block if violence score > 0.5\n        "hate": 0.4      // Block if hate speech score > 0.4\n    }\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"action-execution",children:"Action Execution"}),"\n",(0,s.jsx)(n.p,{children:"Based on the evaluation results, the plugin can take configured actions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n    "actions": {\n        "type": "block",\n        "message": "Content contains inappropriate content."\n    }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"configuration-examples",children:"Configuration Examples"}),"\n",(0,s.jsx)(n.h3,{id:"basic-configuration",children:"Basic Configuration"}),"\n",(0,s.jsx)(n.p,{children:"A simple configuration that enables toxicity detection with default settings:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n    "name": "toxicity_detection",\n    "enabled": true,\n    "stage": "pre_request",\n    "priority": 1,\n    "settings": {\n        "openai_key": "${OPENAI_API_KEY}",\n        "actions": {\n            "type": "block",\n            "message": "Content contains inappropriate content."\n        },\n        "categories": [\n            "sexual",\n            "violence",\n            "hate"\n        ],\n        "thresholds": {\n            "sexual": 0.3,\n            "violence": 0.5,\n            "hate": 0.4\n        }\n    }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"Key components of the basic configuration:"}),"\n",(0,s.jsx)("h4",{align:"center",children:(0,s.jsx)("strong",{children:(0,s.jsx)("u",{children:"Plugin Settings"})})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Property"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"name"})}),(0,s.jsx)(n.td,{children:"Plugin identifier"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:'"toxicity_detection"'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"enabled"})}),(0,s.jsx)(n.td,{children:"Enable/disable plugin"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"true"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"stage"})}),(0,s.jsx)(n.td,{children:"Processing stage"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:'"pre_request"'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"priority"})}),(0,s.jsx)(n.td,{children:"Plugin execution priority"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"1"})]})]})]}),"\n",(0,s.jsx)("h4",{align:"center",children:(0,s.jsx)("strong",{children:(0,s.jsx)("u",{children:"Category Thresholds"})})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Category"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default Threshold"}),(0,s.jsx)(n.th,{children:"Impact"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"sexual"})}),(0,s.jsx)(n.td,{children:"Sexual content detection"}),(0,s.jsx)(n.td,{children:"0.3"}),(0,s.jsx)(n.td,{children:"Lower values = stricter filtering"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"violence"})}),(0,s.jsx)(n.td,{children:"Violence detection"}),(0,s.jsx)(n.td,{children:"0.5"}),(0,s.jsx)(n.td,{children:"Higher values = more permissive"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hate"})}),(0,s.jsx)(n.td,{children:"Hate speech detection"}),(0,s.jsx)(n.td,{children:"0.4"}),(0,s.jsx)(n.td,{children:"Balance based on needs"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"advanced-configuration",children:"Advanced Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Extended configuration with custom thresholds and actions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n    "name": "toxicity_detection",\n    "enabled": true,\n    "stage": "pre_request",\n    "priority": 1,\n    "settings": {\n        "openai_key": "${OPENAI_API_KEY}",\n        "actions": {\n            "type": "block",\n            "message": "Your message was blocked due to inappropriate content. Please revise and try again.",\n            "log_level": "warning"\n        },\n        "categories": [\n            "sexual",\n            "violence",\n            "hate"\n        ],\n        "thresholds": {\n            "sexual": 0.2,\n            "violence": 0.3,\n            "hate": 0.25\n        }\n    }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"The advanced configuration adds:"}),"\n",(0,s.jsxs)(n.p,{children:["\u2022 Custom error messages provide detailed ",(0,s.jsx)(n.strong,{children:"feedback"})," to users when their content is blocked. These messages can be tailored to explain the specific ",(0,s.jsx)(n.strong,{children:"reason"})," for rejection while maintaining a professional tone. The customization allows organizations to align error messaging with their ",(0,s.jsx)(n.strong,{children:"communication"})," style and content policies."]}),"\n",(0,s.jsxs)(n.p,{children:["\u2022 The configuration demonstrates ",(0,s.jsx)(n.strong,{children:"enhanced content moderation"})," by implementing ",(0,s.jsx)(n.strong,{children:"more restrictive thresholds"})," across all categories. These example thresholds (",(0,s.jsx)(n.strong,{children:"0.2"})," for sexual content, ",(0,s.jsx)(n.strong,{children:"0.3"})," for violence, and ",(0,s.jsx)(n.strong,{children:"0.25"})," for hate speech) illustrate potential settings, but should be ",(0,s.jsx)(n.strong,{children:"carefully customized"})," based on your specific ",(0,s.jsx)(n.strong,{children:"use case"})," and ",(0,s.jsx)(n.strong,{children:"requirements"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["\u2022 The implementation includes ",(0,s.jsx)(n.strong,{children:"warning-level"})," logging capabilities that enhance monitoring by tracking blocked content attempts. This system provides comprehensive ",(0,s.jsx)(n.strong,{children:"visibility"})," into filter performance and effectiveness over time. Additionally, the logging system assists administrators in ",(0,s.jsx)(n.strong,{children:"fine-tuning"})," thresholds based on real-world usage patterns and results."]}),"\n",(0,s.jsx)(n.h3,{id:"violence-detection-example",children:"Violence Detection Example"}),"\n",(0,s.jsx)(n.p,{children:"When you need to specifically focus on preventing violent content while allowing other types of content to pass through, you can configure the plugin to only check for violence. This is particularly useful for platforms that prioritize non-violent communication or need to maintain a safe environment for sensitive audiences."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n    "name": "toxicity_detection",\n    "enabled": true,\n    "stage": "pre_request",\n    "priority": 1,\n    "settings": {\n        "openai_key": "${OPENAI_API_KEY}",\n        "actions": {\n            "type": "block",\n            "message": "Your message was blocked because it contains violent content. Please revise your message to avoid violent language.",\n            "log_level": "warning"\n        },\n        "categories": [\n            "violence"\n        ],\n        "thresholds": {\n            "violence": 0.5\n        }\n    }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"This configuration:\n\u2022 Focuses solely on violence detection\n\u2022 Sets a moderate threshold of 0.5 for violent content\n\u2022 Provides a specific error message for violent content\n\u2022 Enables warning-level logging for monitoring"}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"threshold-configuration",children:"Threshold Configuration"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Content Policy Alignment"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Set thresholds according to your content policy"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Consider your audience and use case"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Test thresholds with sample content"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Category Selection"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Enable relevant categories for your use case"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Consider regulatory requirements"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Balance between safety and usability"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance Considerations"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Set appropriate plugin priority"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Consider API rate limits"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Monitor response times"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"API Key Management"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Secure storage of OpenAI API key"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Regular key rotation"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Access control for configuration changes"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Logging and Monitoring"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Enable appropriate logging"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Monitor blocked content patterns"}),"\n",(0,s.jsx)(n.p,{children:"\u2022 Regular threshold adjustments"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsxs)(n.p,{children:["The following example demonstrates how to set up and test a toxicity detection gateway that moderates ",(0,s.jsx)(n.strong,{children:"sexual"}),", ",(0,s.jsx)(n.strong,{children:"violent"}),", and ",(0,s.jsx)(n.strong,{children:"hate speech"})," content using curl commands. We'll first create a gateway with the toxicity detection plugin configured, then test it with two different scenarios: one with safe content and another with content that should be blocked. This example uses default thresholds for all three categories (",(0,s.jsx)(n.strong,{children:"sexual: 0.3"}),", ",(0,s.jsx)(n.strong,{children:"violence: 0.5"}),", ",(0,s.jsx)(n.strong,{children:"hate: 0.4"}),") and demonstrates both successful and blocked requests, helping you understand how the plugin behaves in real-world situations."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Create a gateway with toxicity detection\ncurl -X POST "http://localhost:8080/api/v1/gateways" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "name": "Toxicity Detection Gateway",\n    "subdomain": "toxicity-test",\n    "required_plugins": [\n        {\n            "name": "toxicity_detection",\n            "enabled": true,\n            "stage": "pre_request",\n            "priority": 1,\n            "settings": {\n                "openai_key": "${OPENAI_API_KEY}",\n                "actions": {\n                    "type": "block",\n                    "message": "Content contains inappropriate content."\n                },\n                "categories": ["sexual", "violence", "hate"],\n                "thresholds": {\n                    "sexual": 0.3,\n                    "violence": 0.5,\n                    "hate": 0.4\n                }\n            }\n        }\n    ]\n}\'\n\n# Test with safe content\ncurl -X POST "http://localhost:8081/post" \\\n  -H "Host: toxicity-test.example.com" \\\n  -H "X-API-Key: YOUR_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n        {\n            "role": "user",\n            "content": "Let us discuss dating and relationships in a respectful way"\n        }\n    ]\n}\'\n\n# Test with inappropriate content (will be blocked)\ncurl -X POST "http://localhost:8081/post" \\\n  -H "Host: toxicity-test.example.com" \\\n  -H "X-API-Key: YOUR_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n        {\n            "role": "user",\n            "content": "I will brutally murder you"\n        }\n    ]\n}\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,s.jsxs)(n.p,{children:["The Toxicity Detection plugin employs ",(0,s.jsx)(n.strong,{children:"asynchronous"})," processing to ensure optimal performance during content analysis. By running moderation checks in parallel with other operations, the plugin minimizes the impact on request processing times. This design allows for efficient handling of multiple requests while maintaining ",(0,s.jsx)(n.strong,{children:"responsiveness"})," in high-traffic scenarios."]}),"\n",(0,s.jsxs)(n.p,{children:["To optimize resource utilization, the plugin implements intelligent ",(0,s.jsx)(n.strong,{children:"caching"})," mechanisms and ",(0,s.jsx)(n.strong,{children:"rate limiting"}),". The caching system stores recent moderation results to prevent redundant API calls for similar content, while rate limiting ensures balanced usage of the OpenAI API. These optimizations help maintain consistent performance while managing costs and resource consumption."]}),"\n",(0,s.jsxs)(n.p,{children:["The plugin's architecture is designed with ",(0,s.jsx)(n.strong,{children:"scalability"})," in mind, typically adding only 100-300ms to request processing time. This minimal latency is achieved through efficient memory management and optimized threshold configurations. The system's ",(0,s.jsx)(n.strong,{children:"throughput"})," remains stable even under heavy loads, making it suitable for production environments where performance is critical."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var i=t(96540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);