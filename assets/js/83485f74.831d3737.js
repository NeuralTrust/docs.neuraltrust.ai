"use strict";(self.webpackChunkneuraltrust_docs=self.webpackChunkneuraltrust_docs||[]).push([[2848],{38697:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"trustlens/key-concepts/analytics","title":"Analytics","description":"The Analytics dashboards provides comprehensive insights into your LLM endpoint performance, usage patterns, and security metrics. This powerful observability tool helps you monitor and optimize your AI applications across multiple dimensions.","source":"@site/docs/trustlens/key-concepts/analytics.md","sourceDirName":"trustlens/key-concepts","slug":"/trustlens/key-concepts/analytics","permalink":"/trustlens/key-concepts/analytics","draft":false,"unlisted":false,"editUrl":"https://github.com/NeuralTrust/neuraltrust/blob/main/docs/trustlens/key-concepts/analytics.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Key Concepts","permalink":"/category/key-concepts-2"},"next":{"title":"Tracing","permalink":"/trustlens/key-concepts/tracing"}}');var r=n(74848),t=n(28453);const o={sidebar_position:2},c="Analytics",l={},a=[{value:"Usage Metrics",id:"usage-metrics",level:2},{value:"Topic Classification Metrics",id:"topic-classification-metrics",level:2},{value:"System Performance Metrics",id:"system-performance-metrics",level:2},{value:"User Metrics",id:"user-metrics",level:2},{value:"Security Metrics",id:"security-metrics",level:2},{value:"Feedback Metrics",id:"feedback-metrics",level:2},{value:"Accessibility Metrics",id:"accessibility-metrics",level:2}];function d(e){const s={h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"analytics",children:"Analytics"})}),"\n",(0,r.jsx)(s.p,{children:"The Analytics dashboards provides comprehensive insights into your LLM endpoint performance, usage patterns, and security metrics. This powerful observability tool helps you monitor and optimize your AI applications across multiple dimensions."}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{alt:"Analytics Dashboard|100x50",src:n(1155).A+"",width:"1266",height:"676"})}),"\n",(0,r.jsx)(s.p,{children:"NeuralTrust provides a wide range of metrics to help you monitor and optimize your LLM endpoint."}),"\n",(0,r.jsx)(s.h2,{id:"usage-metrics",children:"Usage Metrics"}),"\n",(0,r.jsx)(s.p,{children:"Track your LLM endpoint utilization through key metrics to optimize resource allocation and understand usage patterns:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Total messages processed"}),": Monitor your application's scale and growth"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of active conversations"}),": Understand concurrent usage and user engagement"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Response generation time"}),": Track how fast your LLM responds to user queries"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Usage patterns over time"}),": Identify trends and cyclical patterns"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Peak usage periods"}),": Prepare for high-demand situations"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"API call frequency"}),": Monitor integration health and usage patterns"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"The current usage metrics are:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of Messages"}),": Total count of individual messages processed by your LLM endpoint"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of Conversations"}),": Total count of unique chat sessions or interactions between users and the LLM"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Dialogue volume"}),": Total amount of text content processed across all conversations"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Dialogue time"}),": Duration of time spent in active conversations with the LLM"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Single message rate"}),": Average time taken to process and respond to individual messages"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Words per message"}),": Average number of words contained in each message or response"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Language"}),": Distribution of languages used across all conversations"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"topic-classification-metrics",children:"Topic Classification Metrics"}),"\n",(0,r.jsx)(s.p,{children:"Understand conversation patterns and user needs through automated topic analysis:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"AI-generated topic clusters"}),": Automatically group similar conversations"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Conversation theme distribution"}),": Visualize popular discussion topics"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Most common use cases"}),": Identify primary user needs"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Topic trend analysis"}),": Track changing user interests over time"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Custom topic mapping"}),": Define and track specific topics of interest"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"The metrics for each custom topic are:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of messages"}),": Total count of individual messages processed by your LLM endpoint"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of conversations"}),": Total count of unique chat sessions or interactions between users and the LLM"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Dialogue time"}),": Duration of time spent in active conversations with the LLM"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Words per prompt"}),": Average number of words in user inputs sent to the LLM"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Words per response"}),": Average number of words in responses generated by the LLM"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Latency"}),": Time taken by the LLM to generate and return a response"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Cost"}),": Financial expense associated with LLM API calls"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Tokens per prompt"}),": Number of tokens (chunks of text) in user inputs, which directly affects API costs"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Tokens per response"}),": Number of tokens in LLM-generated responses, which directly affects API costs"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Readability"}),": Measure of how easy it is to understand the LLM's responses"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"+ Response sentiment"}),": Frequency of positive emotional tone in LLM responses"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"- Response sentiment"}),": Frequency of negative emotional tone in LLM responses"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"+ Prompt sentiment"}),": Frequency of positive emotional tone in user inputs"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"- Prompt sentiment"}),": Frequency of negative emotional tone in user inputs"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"system-performance-metrics",children:"System Performance Metrics"}),"\n",(0,r.jsx)(s.p,{children:"Monitor and optimize your LLM deployment's technical aspects:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Cost per request"}),": Track financial efficiency"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Average latency"}),": Monitor response speed"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Token usage statistics"}),": Optimize prompt and response efficiency"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Model performance metrics"}),": Track accuracy and quality"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Error rates and types"}),": Identify and fix issues"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Resource utilization"}),": Monitor system efficiency"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"The current system performance metrics are:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Calls"}),": Total number of API calls made to your LLM endpoint"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Cost"}),": Total financial expense associated with LLM API calls"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Average latency"}),": Average time taken by the LLM to generate and return a response"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Tokens"}),": Total number of tokens (chunks of text) processed by the LLM"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Tokens per message"}),": Average number of tokens in messages processed by the LLM"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"user-metrics",children:"User Metrics"}),"\n",(0,r.jsx)(s.p,{children:"Understand user behavior and improve engagement:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Active users count"}),": Track user adoption and retention"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Session duration statistics"}),": Understand user engagement depth"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"User engagement metrics"}),": Measure interaction quality"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Return user rates"}),": Monitor user loyalty"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Platform/device usage"}),": Optimize for different platforms"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"The current user metrics are:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Sessions"}),": Total count of unique user sessions or interactions with the LLM"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Users"}),": Total count of unique users who have interacted with your LLM endpoint"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"New Users"}),": Total count of new users who have started using your LLM endpoint"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Sessions per user"}),": Average number of sessions per user"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Sessions by country"}),": Distribution of sessions by country"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Sessions by source"}),": Distribution of sessions by source (e.g., web, mobile, API)"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"security-metrics",children:"Security Metrics"}),"\n",(0,r.jsx)(s.p,{children:"Protect your LLM deployment and ensure safe usage:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Jailbreak attempt detection"}),": Identify security threats"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Content moderation flags"}),": Maintain appropriate content"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Blocked request patterns"}),": Understand security threats"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Security incident trends"}),": Track security health"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"The current security metrics are:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of Prompt data leakage"}),": Count of instances where sensitive information might be exposed in prompts"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of Prompt injection"}),": Count of potential malicious prompt attempts trying to manipulate the LLM's behavior"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"feedback-metrics",children:"Feedback Metrics"}),"\n",(0,r.jsx)(s.p,{children:"Track emotional context and user satisfaction:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Message sentiment scoring"}),": Understand user emotions"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Sentiment trends over time"}),": Monitor changing user attitudes"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Emotional content analysis"}),": Understand user emotional states"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"User satisfaction indicators"}),": Track experience quality"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Response sentiment matching"}),": Ensure appropriate emotional responses"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Conversation tone metrics"}),": Monitor interaction quality"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"The current feedback metrics are:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of positive sentiment"}),": Count of messages with positive emotional tone"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of negative sentiment"}),": Count of messages with negative emotional tone"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"accessibility-metrics",children:"Accessibility Metrics"}),"\n",(0,r.jsx)(s.p,{children:"Ensure your LLM is accessible and understandable:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Message complexity scores"}),": Monitor response clarity"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Language level assessment"}),": Ensure appropriate communication level"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"The current accessibility metrics are:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Readability"}),": Measure of how easy it is to understand the LLM's responses, for elementary school students, middle school students, high school students, college students."]}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},1155:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/analytics-dashboard-76dd6373fc57c7380932a0faba28ad7e.png"},28453:(e,s,n)=>{n.d(s,{R:()=>o,x:()=>c});var i=n(96540);const r={},t=i.createContext(r);function o(e){const s=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function c(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);