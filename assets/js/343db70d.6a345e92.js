"use strict";(self.webpackChunkneuraltrust_docs=self.webpackChunkneuraltrust_docs||[]).push([[6934],{9861:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"sdks/python-sdk/usage/evualation-sets","title":"Evaluation Sets Overview","description":"Evaluation sets are used to assess the performance of your LLM application against defined criteria and metrics. They help you systematically test and measure your application\'s output quality.","source":"@site/docs/sdks/python-sdk/usage/evualation-sets.md","sourceDirName":"sdks/python-sdk/usage","slug":"/sdks/python-sdk/usage/evualation-sets","permalink":"/neuraltrust/sdks/python-sdk/usage/evualation-sets","draft":false,"unlisted":false,"editUrl":"https://github.com/NeuralTrust/neuraltrust/blob/main/docs/sdks/python-sdk/usage/evualation-sets.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Tracing","permalink":"/neuraltrust/sdks/python-sdk/usage/tracing"},"next":{"title":"Knowledge Bases","permalink":"/neuraltrust/sdks/python-sdk/usage/knowledge-bases"}}');var i=t(4848),a=t(8453);const l={sidebar_position:3},r="Evaluation Sets Overview",o={},d=[{value:"1. Creating an Evaluation Set",id:"1-creating-an-evaluation-set",level:2},{value:"2. Running an Evaluation Set",id:"2-running-an-evaluation-set",level:2},{value:"3. Getting Evaluation Set Details",id:"3-getting-evaluation-set-details",level:2},{value:"4. Deleting an Evaluation Set",id:"4-deleting-an-evaluation-set",level:2},{value:"Real-World Example",id:"real-world-example",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"evaluation-sets-overview",children:"Evaluation Sets Overview"})}),"\n",(0,i.jsx)(n.p,{children:"Evaluation sets are used to assess the performance of your LLM application against defined criteria and metrics. They help you systematically test and measure your application's output quality."}),"\n",(0,i.jsx)(n.p,{children:"Here are the key functions and how they work:"}),"\n",(0,i.jsx)(n.h2,{id:"1-creating-an-evaluation-set",children:"1. Creating an Evaluation Set"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'eval_set = client.create_evaluation_set(\n    name="My Eval Set", \n    description="A test evaluation set"\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"This function:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Creates a new evaluation set with a custom name and description"}),"\n",(0,i.jsxs)(n.li,{children:["Returns an ",(0,i.jsx)(n.code,{children:"EvaluationSetResponse"})," object containing the ID of the created set"]}),"\n",(0,i.jsx)(n.li,{children:"Used to organize your test cases into logical groups"}),"\n",(0,i.jsxs)(n.li,{children:["Parameters:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"name"}),": String identifier for the evaluation set"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"description"}),": Detailed description of what this evaluation set tests"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"2-running-an-evaluation-set",children:"2. Running an Evaluation Set"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'eval_set = client.run_evaluation_set(id="eval_set_id")\n'})}),"\n",(0,i.jsx)(n.p,{children:"This function:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Executes all test cases within the specified evaluation set"}),"\n",(0,i.jsx)(n.li,{children:"Collects performance metrics and results"}),"\n",(0,i.jsx)(n.li,{children:"Returns the evaluation results"}),"\n",(0,i.jsxs)(n.li,{children:["Parameters:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"id"}),": The unique identifier of the evaluation set to run"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example with configuration:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Run with custom scheduler\neval_set = client.create_evaluation_set(\n    name="Scheduled Tests",\n    description="Tests that run on a schedule",\n    scheduler="0 0 * * *"  # Runs daily at midnight\n)\n\n# Run the evaluation\nresults = client.run_evaluation_set(id=eval_set.id)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"3-getting-evaluation-set-details",children:"3. Getting Evaluation Set Details"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'eval_set = client.get_evaluation_set(id="eval_set_id")\n'})}),"\n",(0,i.jsx)(n.p,{children:"This function:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Retrieves detailed information about a specific evaluation set"}),"\n",(0,i.jsxs)(n.li,{children:["Returns information including:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Configuration"}),"\n",(0,i.jsx)(n.li,{children:"Test cases"}),"\n",(0,i.jsx)(n.li,{children:"Historical results"}),"\n",(0,i.jsx)(n.li,{children:"Performance metrics"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parameters:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"id"}),": The unique identifier of the evaluation set to retrieve"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"4-deleting-an-evaluation-set",children:"4. Deleting an Evaluation Set"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'client.delete_evaluation_set(id="eval_set_id")\n'})}),"\n",(0,i.jsx)(n.p,{children:"This function:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Permanently removes an evaluation set"}),"\n",(0,i.jsxs)(n.li,{children:["Deletes all associated:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Test cases"}),"\n",(0,i.jsx)(n.li,{children:"Historical results"}),"\n",(0,i.jsx)(n.li,{children:"Configuration"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parameters:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"id"}),": The unique identifier of the evaluation set to delete"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"real-world-example",children:"Real-World Example"}),"\n",(0,i.jsx)(n.p,{children:"Here's a complete example showing how to use evaluation sets:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from neuraltrust import NeuralTrust\n\n# Initialize the client\nclient = NeuralTrust(api_key="your_api_key")\n\n# Create an evaluation set for testing customer service responses\neval_set = client.create_evaluation_set(\n    name="Customer Service Quality",\n    description="Evaluates response quality, accuracy, and tone for customer inquiries"\n)\n\n# Store the evaluation set ID\neval_set_id = eval_set.id\n\n# Run the evaluation\nresults = client.run_evaluation_set(id=eval_set_id)\n\n# Get detailed results\ndetails = client.get_evaluation_set(id=eval_set_id)\n\n# Once no longer needed\nclient.delete_evaluation_set(id=eval_set_id)\n'})}),"\n",(0,i.jsx)(n.p,{children:"The evaluation sets can be used in conjunction with testsets and knowledge bases to create comprehensive testing scenarios for your LLM applications. They help ensure your AI responses maintain quality and consistency across different use cases."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function l(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);