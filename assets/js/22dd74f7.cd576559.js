"use strict";(self.webpackChunkneuraltrust_docs=self.webpackChunkneuraltrust_docs||[]).push([[1567],{5226:t=>{t.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"TrustGate","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Getting Started","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/trustgate/getting-started/overview","docId":"trustgate/getting-started/overview","unlisted":false},{"type":"category","label":"Step-by-Step Guide","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/trustgate/getting-started/step-by-step/overview","docId":"trustgate/getting-started/step-by-step/overview","unlisted":false},{"type":"link","label":"Get AI Gateway","href":"/trustgate/getting-started/step-by-step/get-ai-gateway","docId":"trustgate/getting-started/step-by-step/get-ai-gateway","unlisted":false},{"type":"link","label":"Create Your First Gateway","href":"/trustgate/getting-started/step-by-step/first-gateway","docId":"trustgate/getting-started/step-by-step/first-gateway","unlisted":false},{"type":"link","label":"Configure an Upstream","href":"/trustgate/getting-started/step-by-step/configure-upstream","docId":"trustgate/getting-started/step-by-step/configure-upstream","unlisted":false},{"type":"link","label":"Configure a Service","href":"/trustgate/getting-started/step-by-step/add-service","docId":"trustgate/getting-started/step-by-step/add-service","unlisted":false},{"type":"link","label":"Create Rules","href":"/trustgate/getting-started/step-by-step/create-rules","docId":"trustgate/getting-started/step-by-step/create-rules","unlisted":false},{"type":"link","label":"Load Balancing","href":"/trustgate/getting-started/step-by-step/load-balancing","docId":"trustgate/getting-started/step-by-step/load-balancing","unlisted":false},{"type":"link","label":"Rate Limiting","href":"/trustgate/getting-started/step-by-step/rate-limiting","docId":"trustgate/getting-started/step-by-step/rate-limiting","unlisted":false}],"href":"/trustgate/getting-started/step-by-step"}],"href":"/trustgate/getting-started"},{"type":"category","label":"Key Concepts","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Gateway","href":"/trustgate/concepts/gateway","docId":"trustgate/concepts/gateway","unlisted":false},{"type":"link","label":"Services","href":"/trustgate/concepts/services","docId":"trustgate/concepts/services","unlisted":false},{"type":"link","label":"Upstreams","href":"/trustgate/concepts/upstreams","docId":"trustgate/concepts/upstreams","unlisted":false},{"type":"link","label":"Rules","href":"/trustgate/concepts/rules","docId":"trustgate/concepts/rules","unlisted":false},{"type":"link","label":"Plugins","href":"/trustgate/concepts/plugins","docId":"trustgate/concepts/plugins","unlisted":false},{"type":"link","label":"Traffic Management","href":"/trustgate/concepts/traffic-management","docId":"trustgate/concepts/traffic-management","unlisted":false},{"type":"link","label":"Consumer Groups","href":"/trustgate/concepts/consumer-groups","docId":"trustgate/concepts/consumer-groups","unlisted":false}],"href":"/trustgate/concepts"},{"type":"category","label":"Guides","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Open AI Token-Based Rate Limiting","href":"/trustgate/guides/token-rate-limiting","docId":"trustgate/guides/token-rate-limiting","unlisted":false},{"type":"link","label":"Balancing AI Providers","href":"/trustgate/guides/provider-load-balancing","docId":"trustgate/guides/provider-load-balancing","unlisted":false}],"href":"/trustgate/guides"},{"type":"category","label":"Plugins","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"External API Plugin","href":"/trustgate/plugins/external-api","docId":"trustgate/plugins/external-api","unlisted":false},{"type":"link","label":"Rate Limiting","href":"/trustgate/plugins/rate-limiting","docId":"trustgate/plugins/rate-limiting","unlisted":false},{"type":"link","label":"Token Rate Limiting","href":"/trustgate/plugins/token-rate-limiting","docId":"trustgate/plugins/token-rate-limiting","unlisted":false},{"type":"link","label":"Prompt Moderation","href":"/trustgate/plugins/prompt-moderation","docId":"trustgate/plugins/prompt-moderation","unlisted":false},{"type":"link","label":"Data Masking","href":"/trustgate/plugins/data-masking","docId":"trustgate/plugins/data-masking","unlisted":false},{"type":"category","label":"Toxicity Detection","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OpenAI Toxicity Detection","href":"/trustgate/plugins/toxicity_detection/toxicity-openai-detection","docId":"trustgate/plugins/toxicity_detection/toxicity-openai-detection","unlisted":false},{"type":"link","label":"Azure Toxicity Detection","href":"/trustgate/plugins/toxicity_detection/toxicity-azure-detection","docId":"trustgate/plugins/toxicity_detection/toxicity-azure-detection","unlisted":false}],"href":"/category/toxicity-detection"},{"type":"category","label":"Prompt Guard","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"AWS Bedrock Guardrail","href":"/trustgate/plugins/prompt-guard/bedrock-guardrail","docId":"trustgate/plugins/prompt-guard/bedrock-guardrail","unlisted":false}],"href":"/trustgate/plugins/prompt-guard"},{"type":"link","label":"Injection Protection","href":"/trustgate/plugins/injection-protection","docId":"trustgate/plugins/injection-protection","unlisted":false},{"type":"link","label":"Code Sanitation","href":"/trustgate/plugins/code-sanitation","docId":"trustgate/plugins/code-sanitation","unlisted":false},{"type":"link","label":"Request Size Limiter","href":"/trustgate/plugins/request-size-limiter","docId":"trustgate/plugins/request-size-limiter","unlisted":false}],"href":"/trustgate/plugins"},{"type":"link","label":"Monitoring","href":"/trustgate/monitoring","docId":"trustgate/monitoring","unlisted":false},{"type":"link","label":"Benchmark","href":"/trustgate/benchmark","docId":"trustgate/benchmark","unlisted":false}],"href":"/trustgate"},{"type":"category","label":"TrustTest","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Getting Started","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/trusttest/getting-started/overview","docId":"trusttest/getting-started/overview","unlisted":false},{"type":"category","label":"Step-by-Step Guide","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/trusttest/getting-started/step-by-step/overview","docId":"trusttest/getting-started/step-by-step/overview","unlisted":false},{"type":"link","label":"Configure your LLM endpoint","href":"/trusttest/getting-started/step-by-step/configure-llm-endpoint","docId":"trusttest/getting-started/step-by-step/configure-llm-endpoint","unlisted":false},{"type":"link","label":"Create an EvaluationSet","href":"/trusttest/getting-started/step-by-step/create-evaluation-set","docId":"trusttest/getting-started/step-by-step/create-evaluation-set","unlisted":false},{"type":"link","label":"Run a Custom Objective Attack","href":"/trusttest/getting-started/step-by-step/run-custom-attack","docId":"trusttest/getting-started/step-by-step/run-custom-attack","unlisted":false},{"type":"link","label":"Run a Compliance Scan","href":"/trusttest/getting-started/step-by-step/run-complience-scan","docId":"trusttest/getting-started/step-by-step/run-complience-scan","unlisted":false}],"href":"/trusttest/getting-started/step-by-step"}],"href":"/trusttest/getting-started"},{"type":"category","label":"Key Concepts","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Key Components","href":"/trusttest/key-concepts/overview","docId":"trusttest/key-concepts/overview","unlisted":false},{"type":"link","label":"EvaluationSets","href":"/trusttest/key-concepts/evaluation-sets","docId":"trusttest/key-concepts/evaluation-sets","unlisted":false},{"type":"link","label":"Testsets","href":"/trusttest/key-concepts/testsets","docId":"trusttest/key-concepts/testsets","unlisted":false},{"type":"link","label":"KnowledgeBase","href":"/trusttest/key-concepts/knowledge-bases","docId":"trusttest/key-concepts/knowledge-bases","unlisted":false},{"type":"link","label":"ModelScanner","href":"/trusttest/key-concepts/scanner","docId":"trusttest/key-concepts/scanner","unlisted":false}],"href":"/trusttest/key-concepts"}],"href":"/trusttest"},{"type":"category","label":"TrustLens","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Getting Started","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/trustlens/getting-started/overview","docId":"trustlens/getting-started/overview","unlisted":false},{"type":"category","label":"Step-by-Step Guide","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/trustlens/getting-started/step-by-step/overview","docId":"trustlens/getting-started/step-by-step/overview","unlisted":false},{"type":"link","label":"Track your LLM","href":"/trustlens/getting-started/step-by-step/track-llm","docId":"trustlens/getting-started/step-by-step/track-llm","unlisted":false},{"type":"link","label":"Create a Custom Topic Classifier","href":"/trustlens/getting-started/step-by-step/create-topic-classifier","docId":"trustlens/getting-started/step-by-step/create-topic-classifier","unlisted":false},{"type":"link","label":"Create a Custom Monitor","href":"/trustlens/getting-started/step-by-step/create-custom-monitor","docId":"trustlens/getting-started/step-by-step/create-custom-monitor","unlisted":false}],"href":"/trustlens/getting-started/step-by-step"}],"href":"/trustlens/getting-started"},{"type":"category","label":"Key Concepts","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Analytics","href":"/trustlens/key-concepts/analytics","docId":"trustlens/key-concepts/analytics","unlisted":false},{"type":"link","label":"Tracing","href":"/trustlens/key-concepts/tracing","docId":"trustlens/key-concepts/tracing","unlisted":false},{"type":"link","label":"Monitors","href":"/trustlens/key-concepts/monitors","docId":"trustlens/key-concepts/monitors","unlisted":false}],"href":"/trustlens/key-concepts"}],"href":"/trustlens"},{"type":"category","label":"SDKs","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Python","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Installation","href":"/sdks/python-sdk/installation","docId":"sdks/python-sdk/installation","unlisted":false},{"type":"category","label":"API Reference","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NeuralTrust","href":"/sdks/python-sdk/api-reference/neuraltrust-client","docId":"sdks/python-sdk/api-reference/neuraltrust-client","unlisted":false},{"type":"link","label":"Trace","href":"/sdks/python-sdk/api-reference/trace-class","docId":"sdks/python-sdk/api-reference/trace-class","unlisted":false},{"type":"link","label":"ScannerClient","href":"/sdks/python-sdk/api-reference/scanner-client","docId":"sdks/python-sdk/api-reference/scanner-client","unlisted":false},{"type":"link","label":"TestsetClient","href":"/sdks/python-sdk/api-reference/testset-client","docId":"sdks/python-sdk/api-reference/testset-client","unlisted":false},{"type":"link","label":"EvaluationSetClient","href":"/sdks/python-sdk/api-reference/evaluation-set-client","docId":"sdks/python-sdk/api-reference/evaluation-set-client","unlisted":false},{"type":"link","label":"TraceClient","href":"/sdks/python-sdk/api-reference/trace-client","docId":"sdks/python-sdk/api-reference/trace-client","unlisted":false},{"type":"link","label":"KnowledgeBaseClient","href":"/sdks/python-sdk/api-reference/knowledge-base-client","docId":"sdks/python-sdk/api-reference/knowledge-base-client","unlisted":false}],"href":"/sdks/python-sdk/api-reference"},{"type":"category","label":"Examples","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Functional EvaluationSet Generation","href":"/sdks/python-sdk/usage/functional-evaluation-set","docId":"sdks/python-sdk/usage/functional-evaluation-set","unlisted":false},{"type":"link","label":"Tracing","href":"/sdks/python-sdk/usage/tracing","docId":"sdks/python-sdk/usage/tracing","unlisted":false},{"type":"link","label":"Compliance scan","href":"/sdks/python-sdk/usage/scan","docId":"sdks/python-sdk/usage/scan","unlisted":false},{"type":"link","label":"Custom Attack","href":"/sdks/python-sdk/usage/custom-attack","docId":"sdks/python-sdk/usage/custom-attack","unlisted":false}],"href":"/sdks/python-sdk/usage"}],"href":"/sdks/python-sdk"}],"href":"/sdks"}]},"docs":{"sdks/python-sdk/api-reference/evaluation-set-client":{"id":"sdks/python-sdk/api-reference/evaluation-set-client","title":"EvaluationSetClient","description":"Client for managing EvaluationSets through the NeuralTrust API. This class provides methods for creating, retrieving, updating, deleting, and running EvaluationSets.","sidebar":"tutorialSidebar"},"sdks/python-sdk/api-reference/knowledge-base-client":{"id":"sdks/python-sdk/api-reference/knowledge-base-client","title":"KnowledgeBaseClient","description":"Client for interacting with NeuralTrust Knowledge Base API endpoints. This class provides methods for creating, retrieving, and deleting knowledge bases.","sidebar":"tutorialSidebar"},"sdks/python-sdk/api-reference/neuraltrust-client":{"id":"sdks/python-sdk/api-reference/neuraltrust-client","title":"NeuralTrust","description":"Main client for interacting with the NeuralTrust API. This class provides the main interface for interacting with NeuralTrust services, handling authentication and providing access to various API endpoints.","sidebar":"tutorialSidebar"},"sdks/python-sdk/api-reference/scanner-client":{"id":"sdks/python-sdk/api-reference/scanner-client","title":"ScannerClient","description":"Client for interacting with NeuralTrust\'s scanning and red teaming capabilities. This class provides methods for performing security scans and custom red teaming attacks on LLM systems.","sidebar":"tutorialSidebar"},"sdks/python-sdk/api-reference/testset-client":{"id":"sdks/python-sdk/api-reference/testset-client","title":"TestsetClient","description":"Client for managing testsets through the NeuralTrust API. This class provides methods for creating, retrieving, listing, and deleting testsets.","sidebar":"tutorialSidebar"},"sdks/python-sdk/api-reference/trace-class":{"id":"sdks/python-sdk/api-reference/trace-class","title":"Trace","description":"Class for creating and managing traces in the NeuralTrust system. This class captures details about interactions or events, including metadata, timing information, and content.","sidebar":"tutorialSidebar"},"sdks/python-sdk/api-reference/trace-client":{"id":"sdks/python-sdk/api-reference/trace-client","title":"TraceClient","description":"Client for interacting with the NeuralTrust Trace API. This class provides functionality for creating and managing traces of interactions and events in the NeuralTrust system.","sidebar":"tutorialSidebar"},"sdks/python-sdk/installation":{"id":"sdks/python-sdk/installation","title":"Installation","description":"You can install the Neural Trust Python SDK using pip or other package managers with access to PyPI:","sidebar":"tutorialSidebar"},"sdks/python-sdk/usage/custom-attack":{"id":"sdks/python-sdk/usage/custom-attack","title":"Custom Attack","description":"This guide demonstrates how to create a custom security test using the NeuralTrust API to evaluate AI model responses against specific security objectives.","sidebar":"tutorialSidebar"},"sdks/python-sdk/usage/functional-evaluation-set":{"id":"sdks/python-sdk/usage/functional-evaluation-set","title":"Functional EvaluationSet Generation","description":"This guide explains how to use the NeuralTrust SDK to create and evaluate TestSet for conversational AI applications.","sidebar":"tutorialSidebar"},"sdks/python-sdk/usage/scan":{"id":"sdks/python-sdk/usage/scan","title":"Compliance scan","description":"This guide demonstrates how to use the Neural Trust scanning capabilities to analyze your AI system for potential vulnerabilities.","sidebar":"tutorialSidebar"},"sdks/python-sdk/usage/tracing":{"id":"sdks/python-sdk/usage/tracing","title":"Tracing","description":"This guide demonstrates how to use Neural Trust\'s tracing functionality to monitor and analyze conversations in your AI application.","sidebar":"tutorialSidebar"},"trustgate/benchmark":{"id":"trustgate/benchmark","title":"Benchmark","description":"The AI Gateway has been tested under high load conditions to ensure optimal performance. This guide explains how to run benchmarks and interpret the results.","sidebar":"tutorialSidebar"},"trustgate/concepts/consumer-groups":{"id":"trustgate/concepts/consumer-groups","title":"Consumer Groups","description":"A consumer group in AI Gateway allows you to organize and manage consumers (API users) collectively. This enables you to apply plugins and policies to multiple consumers at once, making it easier to manage access control and rate limiting for different types of users.","sidebar":"tutorialSidebar"},"trustgate/concepts/gateway":{"id":"trustgate/concepts/gateway","title":"Gateway","description":"A gateway in AI Gateway is a logical grouping of configurations that defines how API traffic should be managed, secured, and routed. Each gateway is identified by a unique subdomain and can be configured with its own set of plugins, services, and routing rules.","sidebar":"tutorialSidebar"},"trustgate/concepts/plugins":{"id":"trustgate/concepts/plugins","title":"Plugins","description":"Plugins are modular components that extend and customize AI Gateway\'s functionality. The plugin system follows a stage-based execution model, allowing plugins to process requests and responses at different points in the request lifecycle.","sidebar":"tutorialSidebar"},"trustgate/concepts/rules":{"id":"trustgate/concepts/rules","title":"Rules","description":"A rule in AI Gateway determines how incoming requests are matched and forwarded to services. Rules act as the traffic routing configuration that maps incoming requests to the appropriate services based on paths, methods, and other conditions.","sidebar":"tutorialSidebar"},"trustgate/concepts/services":{"id":"trustgate/concepts/services","title":"Services","description":"In AI Gateway, a service is an entity representing an external upstream API or AI model endpoint. For example, an OpenAI API endpoint, an Anthropic Claude service, or your own custom AI model service.","sidebar":"tutorialSidebar"},"trustgate/concepts/traffic-management":{"id":"trustgate/concepts/traffic-management","title":"Traffic Management","description":"The AI Gateway provides advanced traffic management capabilities to control how requests are distributed across multiple targets, enabling sophisticated load balancing, testing, and deployment strategies.","sidebar":"tutorialSidebar"},"trustgate/concepts/upstreams":{"id":"trustgate/concepts/upstreams","title":"Upstreams","description":"An upstream represents a virtual hostname that can be used to load balance incoming requests across multiple services (targets). In AI Gateway, upstreams define where and how requests are forwarded to backend AI services.","sidebar":"tutorialSidebar"},"trustgate/getting-started/overview":{"id":"trustgate/getting-started/overview","title":"Trust Gate","description":"TrustGate is a robust security and management layer for Large Language Model (LLM) interactions. It provides comprehensive protection, monitoring, and governance for AI applications while enabling organizations to safely deploy and scale their LLM-powered solutions with confidence.","sidebar":"tutorialSidebar"},"trustgate/getting-started/step-by-step/add-service":{"id":"trustgate/getting-started/step-by-step/add-service","title":"Configure a Service","description":"This guide will walk you through adding your first service in TrustGate. A service represents your backend API or AI model endpoint that you want to expose through the gateway.","sidebar":"tutorialSidebar"},"trustgate/getting-started/step-by-step/configure-upstream":{"id":"trustgate/getting-started/step-by-step/configure-upstream","title":"Configure an Upstream","description":"This guide will walk you through configuring your first upstream in TrustGate. An upstream defines how requests are distributed across multiple AI providers and models, or even other backends.","sidebar":"tutorialSidebar"},"trustgate/getting-started/step-by-step/create-rules":{"id":"trustgate/getting-started/step-by-step/create-rules","title":"Create Rules","description":"This guide will walk you through creating rules in TrustGate. Rules determine how incoming requests are matched and routed to your services.","sidebar":"tutorialSidebar"},"trustgate/getting-started/step-by-step/first-gateway":{"id":"trustgate/getting-started/step-by-step/first-gateway","title":"Create Your First Gateway","description":"After installing AI Gateway, the next step is to create your first gateway instance. This guide will walk you through creating and configuring a basic gateway.","sidebar":"tutorialSidebar"},"trustgate/getting-started/step-by-step/get-ai-gateway":{"id":"trustgate/getting-started/step-by-step/get-ai-gateway","title":"Get AI Gateway","description":"TrustGate is available in Open Source. This guide covers the installation methods for the Open Source version.","sidebar":"tutorialSidebar"},"trustgate/getting-started/step-by-step/load-balancing":{"id":"trustgate/getting-started/step-by-step/load-balancing","title":"Load Balancing","description":"Load balancing helps distribute AI model requests across multiple instances for better performance and reliability. TrustGate supports multiple load balancing algorithms and health checking capabilities.","sidebar":"tutorialSidebar"},"trustgate/getting-started/step-by-step/overview":{"id":"trustgate/getting-started/step-by-step/overview","title":"Overview","description":"This guide will walk you through setting up and configuring your first AI Gateway instance. By following these steps, you\'ll learn how to:","sidebar":"tutorialSidebar"},"trustgate/getting-started/step-by-step/rate-limiting":{"id":"trustgate/getting-started/step-by-step/rate-limiting","title":"Rate Limiting","description":"TrustGate provides two complementary rate limiting mechanisms: request-based and token-based rate limiting. These systems work together to protect your AI models and ensure fair resource distribution.","sidebar":"tutorialSidebar"},"trustgate/guides/provider-load-balancing":{"id":"trustgate/guides/provider-load-balancing","title":"Balancing AI Providers","description":"This guide demonstrates how to set up load balancing between multiple AI providers in TrustGate. By distributing traffic across different providers, you can improve reliability, optimize costs, and ensure high availability for your AI applications.","sidebar":"tutorialSidebar"},"trustgate/guides/token-rate-limiting":{"id":"trustgate/guides/token-rate-limiting","title":"Open AI Token-Based Rate Limiting","description":"This guide demonstrates how to implement token-based rate limiting in TrustGate to effectively manage AI model usage and costs. Unlike traditional request-based rate limiting, token-based rate limiting considers the actual token consumption of each request, providing more granular control over API usage.","sidebar":"tutorialSidebar"},"trustgate/monitoring":{"id":"trustgate/monitoring","title":"Monitoring","description":"TrustGate provides comprehensive monitoring capabilities through a Prometheus metrics endpoint. This guide explains how to access and interpret these metrics to monitor your gateway\'s performance and health.","sidebar":"tutorialSidebar"},"trustgate/plugins/code-sanitation":{"id":"trustgate/plugins/code-sanitation","title":"Code Sanitation","description":"Technical Overview","sidebar":"tutorialSidebar"},"trustgate/plugins/data-masking":{"id":"trustgate/plugins/data-masking","title":"Data Masking","description":"Overview","sidebar":"tutorialSidebar"},"trustgate/plugins/external-api":{"id":"trustgate/plugins/external-api","title":"External API Plugin","description":"The External API plugin enables TrustGate to integrate with external validation and transformation services through HTTP endpoints. This plugin is particularly useful for implementing custom validation logic, content moderation, or data transformation workflows.","sidebar":"tutorialSidebar"},"trustgate/plugins/injection-protection":{"id":"trustgate/plugins/injection-protection","title":"Injection Protection","description":"Technical Overview","sidebar":"tutorialSidebar"},"trustgate/plugins/prompt-guard/bedrock-guardrail":{"id":"trustgate/plugins/prompt-guard/bedrock-guardrail","title":"AWS Bedrock Guardrail","description":"Overview","sidebar":"tutorialSidebar"},"trustgate/plugins/prompt-moderation":{"id":"trustgate/plugins/prompt-moderation","title":"Prompt Moderation","description":"Overview","sidebar":"tutorialSidebar"},"trustgate/plugins/rate-limiting":{"id":"trustgate/plugins/rate-limiting","title":"Rate Limiting","description":"TrustGate implements a sophisticated rate limiting system that operates at multiple levels and supports various limiting strategies.","sidebar":"tutorialSidebar"},"trustgate/plugins/request-size-limiter":{"id":"trustgate/plugins/request-size-limiter","title":"Request Size Limiter","description":"Technical Overview","sidebar":"tutorialSidebar"},"trustgate/plugins/token-rate-limiting":{"id":"trustgate/plugins/token-rate-limiting","title":"Token Rate Limiting","description":"TrustGate implements a token bucket-based rate limiting system specifically designed for AI model interactions. This system manages both the number of requests and the token usage per API key.","sidebar":"tutorialSidebar"},"trustgate/plugins/toxicity_detection/toxicity-azure-detection":{"id":"trustgate/plugins/toxicity_detection/toxicity-azure-detection","title":"Azure Toxicity Detection","description":"Overview","sidebar":"tutorialSidebar"},"trustgate/plugins/toxicity_detection/toxicity-openai-detection":{"id":"trustgate/plugins/toxicity_detection/toxicity-openai-detection","title":"OpenAI Toxicity Detection","description":"Technical Overview","sidebar":"tutorialSidebar"},"trustlens/getting-started/overview":{"id":"trustlens/getting-started/overview","title":"What\'s LLM Observability?","description":"LLM Observability is the practice of monitoring, tracking, and analyzing the behavior and performance of Large Language Models (LLMs) in production environments. As organizations increasingly rely on AI and LLMs for critical business operations, having robust observability becomes essential for maintaining reliability, safety, and compliance.","sidebar":"tutorialSidebar"},"trustlens/getting-started/step-by-step/create-custom-monitor":{"id":"trustlens/getting-started/step-by-step/create-custom-monitor","title":"Create a Custom Monitor","description":"Monitor Creation Interface","sidebar":"tutorialSidebar"},"trustlens/getting-started/step-by-step/create-topic-classifier":{"id":"trustlens/getting-started/step-by-step/create-topic-classifier","title":"Create a Custom Topic Classifier","description":"Custom topic classifiers help you categorize conversations based on your specific business needs.","sidebar":"tutorialSidebar"},"trustlens/getting-started/step-by-step/overview":{"id":"trustlens/getting-started/step-by-step/overview","title":"Overview","description":"This guide will walk you through setting up and configuring NeuralTrust to gather data and insights about your LLM applications. By following these steps, you\'ll learn how to:","sidebar":"tutorialSidebar"},"trustlens/getting-started/step-by-step/track-llm":{"id":"trustlens/getting-started/step-by-step/track-llm","title":"Track your LLM","description":"This guide demonstrates how to use Neural Trust\'s tracing functionality to monitor and analyze conversations in your AI application.","sidebar":"tutorialSidebar"},"trustlens/key-concepts/analytics":{"id":"trustlens/key-concepts/analytics","title":"Analytics","description":"The Analytics dashboards provides comprehensive insights into your LLM endpoint performance, usage patterns, and security metrics. This powerful observability tool helps you monitor and optimize your AI applications across multiple dimensions.","sidebar":"tutorialSidebar"},"trustlens/key-concepts/monitors":{"id":"trustlens/key-concepts/monitors","title":"Monitors","description":"Monitoring LLM applications is essential for maintaining performance, controlling costs, and ensuring quality. Through the NeuralTrust platform, you can track key metrics and receive alerts when issues arise.","sidebar":"tutorialSidebar"},"trustlens/key-concepts/tracing":{"id":"trustlens/key-concepts/tracing","title":"Tracing","description":"Comprehensive tracing capabilities for monitoring and debugging your LLM applications.","sidebar":"tutorialSidebar"},"trusttest/getting-started/overview":{"id":"trusttest/getting-started/overview","title":"What\'s Red Teaming for LLMs?","description":"Red teaming is a critical practice for evaluating Large Language Models (LLMs) by systematically challenging their behaviors, safety measures, and potential vulnerabilities. While traditionally associated with security testing, LLM red teaming encompasses both security assessment and functional evaluation of model capabilities.","sidebar":"tutorialSidebar"},"trusttest/getting-started/step-by-step/configure-llm-endpoint":{"id":"trusttest/getting-started/step-by-step/configure-llm-endpoint","title":"Configure your LLM endpoint","description":"This guide explains how to configure your LLM endpoint to invoke through NeuralTrust.","sidebar":"tutorialSidebar"},"trusttest/getting-started/step-by-step/create-evaluation-set":{"id":"trusttest/getting-started/step-by-step/create-evaluation-set","title":"Create an EvaluationSet","description":"This guide explains how to use the NeuralTrust SDK to create and evaluate a functional EvaluationSet from a RAG.","sidebar":"tutorialSidebar"},"trusttest/getting-started/step-by-step/overview":{"id":"trusttest/getting-started/step-by-step/overview","title":"Overview","description":"This guide will walk you through setting up and configuring your first Red Teaming evaluations against your LLM\'s. By following these steps, you\'ll learn how to:","sidebar":"tutorialSidebar"},"trusttest/getting-started/step-by-step/run-complience-scan":{"id":"trusttest/getting-started/step-by-step/run-complience-scan","title":"Run a Compliance Scan","description":"This guide demonstrates how to use the NeuralTrust API to run compliance scans that analyze your AI system for potential vulnerabilities.","sidebar":"tutorialSidebar"},"trusttest/getting-started/step-by-step/run-custom-attack":{"id":"trusttest/getting-started/step-by-step/run-custom-attack","title":"Run a Custom Objective Attack","description":"This guide demonstrates how to create a custom security test using the NeuralTrust API to evaluate AI model responses against specific security objectives.","sidebar":"tutorialSidebar"},"trusttest/key-concepts/evaluation-sets":{"id":"trusttest/key-concepts/evaluation-sets","title":"EvaluationSets","description":"EvaluationSets are a core feature of the NeuralTrust platform that allow you to systematically test and evaluate AI models. They provide a structured way to organize and automate collections of tests to assess model performance, safety, and compliance.","sidebar":"tutorialSidebar"},"trusttest/key-concepts/knowledge-bases":{"id":"trusttest/key-concepts/knowledge-bases","title":"KnowledgeBase","description":"KnowledgeBase are a powerful feature of the NeuralTrust platform that allow you to create and manage specialized knowledge repositories for AI model testing and evaluation. They provide a structured way to organize domain-specific information that can be used to enhance your model testing capabilities.","sidebar":"tutorialSidebar"},"trusttest/key-concepts/overview":{"id":"trusttest/key-concepts/overview","title":"Key Components","description":"NeuralTrust defines the following components to support the above use cases:","sidebar":"tutorialSidebar"},"trusttest/key-concepts/scanner":{"id":"trusttest/key-concepts/scanner","title":"ModelScanner","description":"The Model Scanner is a crucial tool for evaluating and ensuring the safety and compliance of Large Language Models (LLMs). As LLMs become increasingly integrated into production systems, it\'s essential to proactively identify and mitigate potential risks related to harmful, biased, or malicious content generation.","sidebar":"tutorialSidebar"},"trusttest/key-concepts/testsets":{"id":"trusttest/key-concepts/testsets","title":"Testsets","description":"Testsets are a fundamental component of the NeuralTrust platform that enable you to create and manage collections of test cases for AI model evaluation. Each test set contains multiple query-response pairs, where each pair consists of:","sidebar":"tutorialSidebar"}}}}')}}]);