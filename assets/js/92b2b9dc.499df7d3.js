"use strict";(self.webpackChunkneuraltrust_docs=self.webpackChunkneuraltrust_docs||[]).push([[6002],{1523:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"observability/key-concepts/monitors","title":"Monitors","description":"Monitoring LLM applications is essential for maintaining performance, controlling costs, and ensuring quality. Through the NeuralTrust platform, you can track key metrics and receive alerts when issues arise.","source":"@site/docs/observability/key-concepts/monitors.md","sourceDirName":"observability/key-concepts","slug":"/observability/key-concepts/monitors","permalink":"/observability/key-concepts/monitors","draft":false,"unlisted":false,"editUrl":"https://github.com/NeuralTrust/neuraltrust/blob/main/docs/observability/key-concepts/monitors.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Tracing","permalink":"/observability/key-concepts/tracing"},"next":{"title":"SDKs","permalink":"/category/sdks"}}');var r=n(4848),i=n(8453);const o={sidebar_position:4},a="Monitors",l={},c=[{value:"Types of Monitors",id:"types-of-monitors",level:2},{value:"Metrics",id:"metrics",level:2},{value:"Priority Levels",id:"priority-levels",level:2}];function d(e){const s={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"monitors",children:"Monitors"})}),"\n",(0,r.jsx)(s.p,{children:"Monitoring LLM applications is essential for maintaining performance, controlling costs, and ensuring quality. Through the NeuralTrust platform, you can track key metrics and receive alerts when issues arise."}),"\n",(0,r.jsx)(s.h2,{id:"types-of-monitors",children:"Types of Monitors"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Metric Monitor"}),": Metric monitors compare metric values against a static threshold. During each alert evaluation, NeuralTrust calculates the selected metric over the selected period and checks if it's above/below the threshold. This is the standard alerting case where you know what unexpected values look like."]}),"\n",(0,r.jsx)(s.p,{children:"For example, you might want to be alerted when the average response time exceeds 5 seconds or when the total cost goes above a certain budget threshold."}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Change Monitor"}),": Change monitors evaluate the difference between a past value and the current value. During each alert evaluation, NeuralTrust calculates the difference between the current series and the series from N minutes ago, then computes the selected metric over the selected period. An alert is triggered when this computed series crosses the threshold."]}),"\n",(0,r.jsx)(s.p,{children:"Change monitors evaluate the difference between a past value and the current value. During each alert evaluation, NeuralTrust calculates the difference between the current series and the series from N minutes ago, then computes the selected metric over the selected period. An alert is triggered when this computed series crosses the threshold."}),"\n",(0,r.jsx)(s.p,{children:"This type of monitor is particularly useful for detecting sudden changes in your metrics, such as an unexpected spike in error rates or a significant drop in successful completions."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"metrics",children:"Metrics"}),"\n",(0,r.jsx)(s.p,{children:"The metric to monitor and the one that will be used to trigger the alert. Currently, we support the following metrics:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of Messages"}),": Total count of individual prompts and responses exchanged with the LLM."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of Conversations"}),": Count of distinct chat sessions or interactions with the LLM."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Dialog time"}),": Duration of the entire conversation from start to finish."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Words per prompt"}),": Average number of words in user inputs sent to the LLM."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Words per response"}),": Average number of words in responses generated by the LLM."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Prompt language"}),": Detected language of the user inputs."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Latency"}),": Time taken by the LLM to generate and return a response."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Cost"}),": Financial expense associated with LLM API calls."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Tokens per prompt"}),": Number of tokens (chunks of text) in user inputs, which directly affects API costs."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Tokens per response"}),": Number of tokens in LLM-generated responses, which directly affects API costs."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Readability"}),": Measure of how easy it is to understand the LLM's responses."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"+ Response sentiment"}),": Frequency of positive emotional tone in LLM responses."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"- Response sentiment"}),": Frequency of negative emotional tone in LLM responses."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"+ Prompt sentiment"}),": Frequency of positive emotional tone in user inputs."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"- Prompt sentiment"}),": Frequency of negative emotional tone in user inputs."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of Prompt data leakage"}),": Count of instances where sensitive information might be exposed in prompts."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Number of Prompt injection"}),": Count of potential malicious prompt attempts trying to manipulate the LLM's behavior."]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"priority-levels",children:"Priority Levels"}),"\n",(0,r.jsx)(s.p,{children:"Alerts are categorized by priority levels to help teams quickly assess and respond to issues:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"P1 Critical"}),": Severe incidents requiring immediate attention that directly impact system availability or security. Examples include system outages, security breaches, or critical performance degradation."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"P2 High"}),": Urgent issues that significantly affect system performance or user experience but don't cause complete service disruption. Examples include significant latency increases or high error rates."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"P3 Medium"}),": Important issues that should be addressed soon but don't require immediate action. Examples include moderate cost increases or declining performance metrics."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"P4 Low"}),": Minor issues that should be monitored but don't impact system functionality. Examples include slight variations in response times or small increases in token usage."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"P5 Info"}),": Informational alerts for tracking system changes or trends. Examples include routine usage statistics or planned maintenance notifications."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,i.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>o,x:()=>a});var t=n(6540);const r={},i=t.createContext(r);function o(e){const s=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(i.Provider,{value:s},e.children)}}}]);